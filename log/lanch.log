[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
-----------  Configuration Arguments -----------
batch_size: 3000
beam_size: 5
beta1: 0.9
beta2: 0.998
block_trigram: True
candidate_sentence_num: 10
candidate_summary_num: 10
checkpoints: checkpoints
config_path: model_config/roberta_graphsum_config.json
data_path: src/data_preprocess/input_data
dataset: test
decode_path: roberta_results/
decr_every_n_nan_or_inf: 2
decr_ratio: 0.8
dev_set: ./data/examples_dataset/valid
do_dec: True
do_lower_case: True
do_preprocessing: True
do_test: True
do_train: False
do_val: False
encoder_json_file: src/data_preprocess/config/bpe/gpt2_bpe/encoder.json
epoch: 100
eps: 1e-08
ernie_config_path: ernie_config/ernie_config.json
ernie_vocab_file: ernie_config/vocab.txt
evaluate_blue: False
find_opt_num: False
grad_norm: 2.0
graph_type: similarity
in_tokens: True
incr_every_n_steps: 100
incr_ratio: 2.0
init_checkpoint: multinews_extra/
init_loss_scaling: 12800.0
init_pretraining_params: ./roberta_config/roberta.base
is_distributed: False
json_path: src/data_preprocess/json_data/
label_selection_type: greedy_selection
label_smooth_eps: 0.1
learning_rate: 0.03
len_penalty: 0.6
log_file: log/cnndm_test.log
lower: True
lr_scheduler: noam_decay
max_doc_num: 20
max_nsents: 100
max_out_len: 300
max_para_len: 768
max_para_num: 100
max_seq_len: 512
max_src_ntokens: 768
max_tgt_len: 300
metrics: True
min_nsents: 5
min_out_len: 200
min_src_ntokens: 5
mode: 
model_name: multigraphsum
n_cpus: 20
num_iteration_per_drop_scope: 10
num_topics: 20
pos_win: 2.0
process_wiki: False
random_seed: 1
report_orcale: False
report_rouge: True
roberta_config_path: /content/EMNLP2021-SgSum/roberta_config/roberta_config.json
roberta_vocab_file: src/data_preprocess/config/bpe/gpt2_bpe/vocab.txt
save_steps: 10000
selected_sentence_num: 9
shard_size: 2000
sim_function: tf-idf
sim_threshold: 0.1
skip_steps: 100
stream_job: 
summary_sent_num: 30
test_set: ./data/examples_dataset/test
test_src: src/data_preprocess/data/example.source
test_tgt: src/data_preprocess/data/example.target
train_set: ./data/examples_dataset/train
use_cuda: False
use_dynamic_loss_scaling: False
use_fast_executor: True
use_fp16: False
use_interval: False
use_multi_gpu_test: False
validation_steps: 20000
verbose: True
vocab_bpe_file: src/data_preprocess/config/bpe/gpt2_bpe/vocab.bpe
vocab_path: src/data_preprocess/config/vocab/spm9998_3.model
warmup_proportion: 0.1
warmup_steps: 10000
weight_decay: 0.01
weight_sharing: True
------------------------------------------------
1.933331
Written test.0.json
2.009403
2.009438
Ignore src/data_preprocess/input_data/test.0.json
2.079105
attention_probs_dropout_prob: 0.1
dec_graph_layers: 6
dec_word_pos_embedding_name: dec_word_pos_embedding
enc_graph_layers: 2
enc_sen_pos_embedding_name: enc_sen_pos_embedding
hidden_act: relu
hidden_dropout_prob: 0.1
initializer_range: 0.02
postprocess_command: da
preprocess_command: n
word_embedding_name: word_embedding
------------------------------------------------
[2021-12-26 14:24:45,133 INFO] {'BOS': 0, 'EOS': 50264, 'PAD': 1, 'EOQ': 2}
attention_probs_dropout_prob: 0.1
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
initializer_range: 0.02
max_position_embeddings: 770
num_attention_heads: 12
num_hidden_layers: 12
sent_type_vocab_size: 0
task_type_vocab_size: 0
type_vocab_size: 0
vocab_size: 50265
------------------------------------------------
Traceback (most recent call last):
  File "src/run_roberta.py", line 53, in <module>
    run_multigraphsum(args)
  File "/content/SgSum/src/networks/roberta_multigraphextsum/run_graphsum.py", line 95, in main
    vocab_size=vocab_size)
  File "/content/SgSum/src/networks/roberta_multigraphextsum/graphsum_model.py", line 117, in __init__
    self.sent_score_threshold = args.sent_score_threshold
AttributeError: 'Namespace' object has no attribute 'sent_score_threshold'
